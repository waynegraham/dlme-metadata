require 'csv'
require 'digest'
require 'fileutils'
require 'json'
require 'open-uri'
require 'pp'

require 'colorize'
require 'mechanize'

task default: %w[all]

task :all => %w[import:openn]

namespace :import do

  namespace :princeton do
    task all: %w[import:princeton:islamicmss import:princeton:ymdi]

    task :stats do
      Dir.glob('records/princeton/**/*.json').each do |file|
        json = JSON.parse(File.read(file))
        json['metadata'].each do |obj|
          pp "#{obj['label']}: #{obj['value']}"
        end
        break
      end
    end

    desc 'Parse the JSON files to ensure they are valid'
    task :parse_json do
      files = Dir.glob('princeton_*.txt')

      errors = []

      files.each do |file|
        cache_path = "records/princeton/#{File.basename(file, '.txt')}"

        Dir.glob("#{cache_path}/*.json").each do |json|
          puts "Reading #{json}".green

          begin
            json_file = JSON.parse(File.read(json))
          rescue
            puts "\tError parsing #{json}".red
            errors << json
          end

        end
      end

      puts "\nFiles with errors:\n"
      puts errors
      puts "\nRemoving files:\n"

      errors.each do |file|
        FileUtils.rm(file)
      end

      puts "Make sure you run the import scripts again to redownload missing files".colorize(color: :white, background: :red).underline
    end

    def process_ark_file(file)
      prefix = 'https://plum.princeton.edu/iiif/lookup/'
      path =  "records/princeton/#{File.basename(file, '.txt')}"
      FileUtils.mkdir_p(path)

      file = CSV.read(file)
      total = file.length
      counter = 0

      file.each do |row|
        ark = row.first
        resource = "#{prefix}#{ark}"
        file = /[^\/]+$/.match(ark).to_s

        counter += 1
        percentage = (counter / total.to_f) * 100

        unless File.exist?("#{path}/#{file}.json")
          puts "Fetching #{file} (#{sprintf('%.02f', percentage)}%)".green
          open("#{path}/#{file}.json", 'w') do |file|
            file << open(resource).read
          end
        else
          puts "Skipping #{file} (#{sprintf('%.02f', percentage)}%)".yellow
        end
      end
    end

    desc 'Cache missing manifests from the Princeton Islamic Manuscripts Collection'
    task :islamicmss do
      process_ark_file('./princeton_islamicmss.txt')
    end

    desc 'Cache missing manifests from the Princeton Yemeni Manuscripts Collection'
    task :ymdi do
      process_ark_file('./princeton_ymdi.txt')
    end
  end

  task openn: %w[schoenberg galen]

  desc 'Import Selected items from the Schoenberg collection'
  task :schoenberg do
    output_path = 'records/penn/schoenberg'
    FileUtils.mkdir_p output_path

    ids = %W(189 196 235 286 293 294 295 296 299 311 312 322 37 38 387 388 398 399 40 400 403 404 405 407 408 409 410 412 414 417 425 426 427 434 435 436 441 444 447 45 455 456 459 460 464 467 469 486 489 49 495)

    ids.each do |id|
      url = "http://openn.library.upenn.edu/Data/0001/ljs#{id}/data/ljs#{id}_TEI.xml"
      puts "Fetching #{url}"
      document = Nokogiri::XML(open(url))

      puts "Writing #{output_path}/ljs#{id}.xml file"
      File.open("#{output_path}/ljs#{id}.xml", 'w') { |file| file.write(document) }
    end
  end

  desc "Import Syriac Palimpset Folio"
  task :galen do
    # Description: http://openn.library.upenn.edu/Data/0014/GalenSyriacPalimpsest/0_ReadMe.txt
    # Folio Index: http://openn.library.upenn.edu/Data/0014/GalenSyriacPalimpsest/Documents/Internal/FolioIndex.html

  end



end
